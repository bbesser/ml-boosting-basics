{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting für Machine Learning\n",
    "\n",
    "Wir beschäftigen uns mit den grundlegenden Varianten des sogenannten _Boostings_.\n",
    "Dabei legen wir unser Augenmerk auf Boosting für die Klassifikation.\n",
    "Insbesondere klassifizieren wir Reden von deutschen Politikern, d.h. aus einem gegebenen Redetext ermitteln wir wer diese Rede hielt.\n",
    "(Diese Problemstellung ist auch Thema im Abschnitt [Natural Language Processing](https://www.youtube.com/watch?v=GmLsb-o7hvM) dieses [Bootcamps](https://www.codecentric.de/kuenstliche-intelligenz/).)\n",
    "\n",
    "Boosting dreht sich um die folgende Frage.\n",
    "\n",
    "_Angenommen für unser Problem steht ein schlechter Klassifikator K zur Verfügung._\n",
    "_Wie können wir aus K einen guten Klassifikator K' erzeugen?_\n",
    "\n",
    "Unter einem schlechten Klassifikator verstehen wir einen Klassifikator mit schlechter Vorhersagekraft, d.h. er macht viele falsche Vorhersagen.\n",
    "(Die Theorie erlaubt sogar so viele Fehler, dass die Kraft nur wenig besser als zufälliges Raten ist!)\n",
    "Der gute Klassifikator K' wird keine abgewandelte Form von K sein, sondern K' besteht aus mehreren geschickt kombinierten Instanzen von K.\n",
    "Daher bezeichnet man K' auch als _Ensemble_.\n",
    "\n",
    "Ensembles lassen sich aus unterschiedlichen Typen von Klassifikatoren zusammenstellen.\n",
    "Hier beschränken wir uns auf den Fall, dass K ein einfacher [Entscheidungsbaum](https://en.wikipedia.org/wiki/Decision_tree_learning) ist.\n",
    "Eine Vorhersage des Ensembles K' ergibt sich aus den individuellen Vorhersagen der beteiligten Entscheidungsbäume.\n",
    "\n",
    "Im Rest des Artikels stellen wir verschiedene Boosting-Ansätze je kurz in allgemeiner Form vor, orientieren jedoch nähere Erläuterungen an der Implementierung mit Entscheidungsbäumen.\n",
    "Bemerke, dass wir kein Tuning der Implementierung durchführen, sondern uns vielmehr darauf konzentrieren, die Unterschiede der Verfahren zu beleuchten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten laden\n",
    "\n",
    "Werfen wir zuerst einen Blick auf die Daten.\n",
    "Wir verwenden den von Barbaresi und Adrien bereitgestellten Datensatz [1], der unter http://purl.org/corpus/german-speeches ist.\n",
    "\n",
    "[1] Barbaresi, Adrien (2018). \"A corpus of German political speeches from the 21st century\", Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), European Language Resources Association (ELRA), pp. 792–797.\n",
    "\n",
    "Vorab ein Einblick in wenige zufällig ausgewählte Reden.\n",
    "(Hier wie auch unten lagern wir Code aus, welcher nicht zu unserem Lernziel beiträgt, und binden ihn per `%run` ein.)\n",
    "Die Variable `df` ist ein `DataFrame` aus der [Pandas](https://pandas.pydata.org/)-Bibliothek;\n",
    "Im Wesentlichen werden in `df` tabellarische Daten gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run includes/define_load_data_functions.ipynb\n",
    "\n",
    "df = load_cached_speeches_or_download()\n",
    "\n",
    "display(df.sample(n=5)) # `sample` wählt eine zufällige Teilmenge aus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten die Spalte `person` und stellen fest, dass Angela Merkel mit der weitaus stärksten Anzahl von Reden vertreten ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "speech_counts = df.person.value_counts()\n",
    "\n",
    "display(speech_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um im restlichen Notebook auf ausgewogenen Daten zu arbeiten, beschränken wir uns auf die binäre Klassifikation und unterscheiden nur zwischen zwei Klassen von Personen, nämlich _Angela Merkel_ und _Andere_.\n",
    "Dazu vernachlässigen wir zuerst alle Personen, die mit nur wenigen Reden vertreten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NUM_SPEECHES = 100\n",
    "belowThreshold = lambda name: speech_counts[name] < MIN_NUM_SPEECHES\n",
    "\n",
    "dropped = filter(belowThreshold, df.person.tolist())\n",
    "df.drop(df[df.person.isin(dropped)].index, inplace=True)\n",
    "\n",
    "display(df.person.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abschließend fassen wir alle Personen außer Angela Merkel zur Klasse _Andere_ zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['person'] != \"Angela Merkel\", ['person']] = 'Andere'\n",
    "\n",
    "display(df.person.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vorverarbeiten\n",
    "\n",
    "Es folgt eine Vorverarbeitung der Reden mit Hilfe der NLP-Bibliothek [spaCy](https://spacy.io/).\n",
    "In diesem Schritt zerlegen wir jede Rede in ihre (durch Whitespace getrennten) Bestandteile, die sogenannten _Tokens_.\n",
    "In dem Zuge entfernen wir Tokens mit geringer Information, wie z.B. Interpunktion und Stoppwörter (\"und\", \"der\", \"die\", \"das\", ...).\n",
    "Außerdem überführen wir jedes Token in seine Grundform, das sogenannte _Lemma_ (z.B. \"angekündigt\" -> \"ankündigen\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%run includes/init_spacy.ipynb\n",
    "%run includes/define_preprocessing.ipynb\n",
    "\n",
    "df = load_cached_or_preprocess(df.speech)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jede Rede R überführen wir nun die Folge von Lemmata von R in das \"Histogramm\" der Lemmahäufigkeiten von R.\n",
    "Bemerke, dass wir dabei die Reihenfolge der Worte vergessen.\n",
    "(Man nennt solch eine vereinfachte Darstellung einer Wortfolge auch _bag of words_, um das Abhandensein von zeitlicher Information zu betonen.)\n",
    "Dann schauen wir uns ein paar Lemmata und die gewonnene Darstellung einer Rede im Detail an.\n",
    "\n",
    "(Einige Lemmata sind nicht \"perfekt\".\n",
    "Sie enthalten etwa Bindestriche oder liegen in gebeugter Form vor.\n",
    "Hier wird die Unschäfe der Sprache und ihrer Verarbeitung deutlich.\n",
    "Beispielsweise geschieht Lemmatisierung in spaCy nicht durch Anwendung eines Regelwerks, sondern durch neuronale Netze, die natürlich nicht in allen Fällen korrekte Ergebnisse erzielen.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%run includes/define_feature_computation.ipynb\n",
    "\n",
    "(lemmata, lemma_index) = compute_index(df[\"lemmata\"])\n",
    "df[\"lemma_counts\"] = count(df[\"lemmata\"], lemma_index)\n",
    "\n",
    "display(lemmata[0:10])\n",
    "display(df.iloc[11]) # In lemma_counts ist jedes Lemma durch eine Nummer repräsentiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Bis hier haben wir aus den Reden die für das Modell-Training benötigten statistischen Informationen extrahiert.\n",
    "Zum Start des Trainings fehlt nur noch die Konvertierung in das benötigte Eingabedatenformat.\n",
    "\n",
    "Zum Training nutzen wir die ML-Bibliothek [scikit-learn](https://scikit-learn.org/stable/).\n",
    "Die eingegebenen Trainingdaten bestehen aus einer Sammlung von sogenannten _Labels_ (eins pro Rede) und sogenannten _Featurevektoren_ (ebenfalls einer pro Rede).\n",
    "In unserem Fall gibt das Label einer Rede R an, welche Person die Rede R hielt (_Angela Merkel_ oder _Andere_).\n",
    "Der Feature-Vektor von R kodiert das Histogramm der Lemmahäufigkeiten von R (haben wir oben bereits berechnet).\n",
    "\n",
    "Für die Eingabe in die Algorithmen von scikit-learn werden alle Labels in einen Vektor zusammengefasst.\n",
    "Analog werden alle Featurevektoren zu einer Matrix zusammengefasst.\n",
    "Dafür müssen natürlich alle Featurevektoren die gleiche Länge haben, was wir mit Hilfe der Funktion `dict_to_sparse` erledigen.\n",
    "(Insbesondere gibt diese Funktion eine sogenannte _dünn besetzte Matrix_ aus, in der Einträge mit Wert Null nicht explizit gespeichert werden.\n",
    "Warum?\n",
    "In einer gegebenen Rede erscheinen bei Weitem nicht alle möglichen Lemmata, folglich enthält jeder Featurevektor viele Nullen.\n",
    "Folglich ist auch die Matrix voller Nullen.\n",
    "Ohne explizite Darstellung sparen wir also viel Speicherplatz.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%run includes/define_conversion_functions.ipynb\n",
    "\n",
    "feature_vectors = dict_to_sparse(df[\"lemma_counts\"], len(lemma_index))\n",
    "categories = df[\"person\"].astype(\"category\")\n",
    "labels = categories.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möge das Training beginnen.\n",
    "Wir erinnern uns, dass wir Ensembles von schwachen Klassifizierern erstellen möchten.\n",
    "Als schwachen Klassifizierer wählen wir einen sogenannten _Decision Stump_, d.h. einen Entscheidungsbaum der Tiefe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DECISION_TREE_DEPTH=1\n",
    "\n",
    "def generate_decision_stump():\n",
    "    return DecisionTreeClassifier(max_depth=DECISION_TREE_DEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Warmwerden trainieren wir zunächst einen einzelnen Stump.\n",
    "\n",
    "Dafür werden unsere Eingabedaten in Trainingdaten und Testdaten zerlegt, und zwar im üblichen Verhältnis 70/30.\n",
    "Nur die Trainingsdaten werden für das tatsächliche Training verwendet.\n",
    "Mit Hilfe der Testdaten wird die Performance des fertig trainierten Stumps evaluiert.\n",
    "\n",
    "Damit wir die Performance verlässich evaluieren können, führen wir das Experiment nicht nur einmal durch, sondern wiederholen es 20 Mal und mitteln die Ergebnisse.\n",
    "Die _Accuracy_ zeigt uns wie viele Testdatensätze korrekt klassifiziert wurden.\n",
    "Die sogenannte _Confusion Matrix_ schlüsselt die korrekten und falschen Klassifizierungen auf (in der i-ten Zeile und j-ten Spalte steht wie viele Reden von \"i\" als Reden von \"j\" klassifiziert wurden).\n",
    "Wir sehen, dass nicht einmal 80% der Testdaten korrekt klassifiziert werden, wobei mehr Reden von Angela Merkel falsch klassifiziert wurden (über 30%) als Reden von anderen (ca. 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run includes/define_plot_functions.ipynb\n",
    "%run includes/define_train_functions.ipynb\n",
    "\n",
    "TRAIN_TEST_RATIO = 0.3\n",
    "REPETITIONS = 20\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_decision_stump, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werfen einen Blick in die Blackbox und betrachten einen der trainierten Decision Stumps genauer.\n",
    "Interessant ist insbesondere die erste Zeile des Wurzelknotens.\n",
    "Die Häufigkeit genau eines Lemmas bestimmt welchem der beiden Blätter eine Rede zugeordnet wird.\n",
    "Jedes Blatt klassifiziert alle ihm zugeordneten Reden mit der gleichen Person, wie die jeweils letzte Zeile zeigt.\n",
    "\n",
    "(Die Werte _samples_ und _value_ zeigen wie viele Reden dem jeweiligen Knoten zugeordnet werden bzw. die vom Knoten berechnete Redner-Verteilung aller dem Knoten zugeordneten Reden.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_decision_tree(classifiers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beachte, dass dieser Stump einen Schnitt durch einen n-dimensionalen Raum darstellt, wobei n die Anzahl der Lemmata ist.\n",
    "Eine Visualisierung durch bspw. eine Fläche ist also nicht möglich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Bisher haben wir mehrere Stumps unabhängig voneinander trainiert und ihre mittlere Performance betrachtet.\n",
    "Diese Idee verfolgt auch das sogenannte _Bagging_ (kurz für _bootstrap aggregating_).\n",
    "Bagging erstellt ebenfalls ein Ensemble von unabhängigen Stumps, wendet die Mittelung jedoch schon früher an, nämlich zur Klassifizierung einer Rede.\n",
    "\n",
    "Wir erstellen ein Ensemble von 20 unabhängigen Decision Stumps, wobei für jeden eine zufällige Teilmenge der Trainingsdaten verwendet wird.\n",
    "Da die Berechnungen der individuellen Stumps nicht voneinander abhängen, können wir mit Hilfe von allen zur Verfügung stehenden CPUs parallelisieren.\n",
    "\n",
    "<img src=\"images/20190102_151532.jpg\" width=\"50%\">\n",
    "\n",
    "Das Bagging-Ensemble kombiniert die Vorhersagen der individuellen Stumps zu einer einzigen Vorhersage.\n",
    "Wenn die Person für Rede R vorhergesagt werden soll, wird nicht einfach nur unter allen individuellen Vorhersagen abgestimmt, sondern die individuellen Vorhersagewahrscheinlichkeiten beider Personen für R gemittelt und dieses Mittel zur finalen Vorhersage verwendet.\n",
    "\n",
    "<img src=\"images/20190102_151541.jpg\" width=\"50%\">\n",
    "\n",
    "Bemerke, dass wir keine Verbesserung der Performance gegenüber unserem Experiment aus dem vorangehenden Abschnitt erzielen.\n",
    "Grund ist, dass jeder Bagging-Klassifizierer im Wesentlichen unsere Mittelung nachstellt.\n",
    "Die Wiederholung des Baggings und die anschließende Mittelung über die Performances der Ensembles verändert die einzelnen Ergebnisse nicht entscheidend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "NUM_BASE_ESTIMATORS = 20\n",
    "ALL_CORES = -1\n",
    "\n",
    "def generate_bagging_classifier():\n",
    "    return BaggingClassifier(base_estimator=generate_decision_stump(),\n",
    "                             n_estimators=NUM_BASE_ESTIMATORS,\n",
    "                             n_jobs=ALL_CORES)\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_bagging_classifier, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Die Performance des schwachen Decision Stumps und des naiven Baggings können wir sicherlich verbessern.\n",
    "Erfreulich wird sein, mit welch einfachen Mitteln das gelingt!\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) erstellt ebenfalls ein Ensemble von schwachen Klassifizierern.\n",
    "Die Klassifizierer sind im Gegensatz zum Bagging aber nicht unabhängig voneinander.\n",
    "Erstellt wird nämlich eine _Folge_ von Klassifizierern, wobei sich die Performance eines Klassifizierers auf das Training des nächsten auswirkt.\n",
    "Ziel ist es, Schwächen des Klassifizierers zu identifizieren, diese Schwächen in weiteren Trainings zu kompensieren und schließlich stark aus dem finalen Trainingsplan hervorzugehen.\n",
    "\n",
    "Wie trainiert AdaBoost in unserem Fall?\n",
    "Für jeden fertig trainierten Stump S werden falsch klassifizierte Reden identifiziert.\n",
    "Diese Reden stellen offenbar \"schwierige\" Eingaben für S dar - sonst würde sie ja von S korrekt zugeordnet.\n",
    "Solch schwierige Reden werden mit erhöhter Wahrscheinlichkeit ausgewählt, wenn eine zufällige Trainingsmenge für den nächsten Stump erzeugt wird.\n",
    "Das kommende Training ist also mit einer \"schwierigeren\" Eingabe konfrontiert, welche mehr Informationen über die Gesamtmenge aller Reden innehat.\n",
    "(Tatsächlich wird die Auswahlwahrscheinlichkeit _korrekt_ von S klassifizierter Reden _verringert_.\n",
    "Der Verringerungsfaktor ergibt sich aus dem _Fehler von S_, der Wahrscheinlichkeit eine zufällig gemäß aktueller Verteilung gezogene Rede falsch zu klassifizieren.)\n",
    "\n",
    "<img src=\"images/20190102_172653.jpg\" width=\"50%\">\n",
    "\n",
    "Wie klassifiziert AdaBoost?\n",
    "Jeder Stump des Ensembles beeinflusst die Gesamtvorhersage mit unterschiedlichem Gewicht.\n",
    "Das Gewicht für Stump S ergibt sich aus dem Fehler von S:\n",
    "Je kleiner der Fehler, umso größer der Einfluss von S.\n",
    "\n",
    "<img src=\"images/20190102_175314.jpg\" width=\"50%\">\n",
    "\n",
    "Das Ergebnis unseres Experiments:\n",
    "Im Schnitt erreicht ein AdaBoost-Klassifier eine Genauigkeit von über 90% (mit Bagging erreichten wir nur unter 80%).\n",
    "Fehlklassifizierungen liegen nun im einstelligen Prozentbereich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# keine parallele Berechnung der Sequenz möglich\n",
    "def generate_adaboost_classifier():\n",
    "    return AdaBoostClassifier(base_estimator=generate_decision_stump(),\n",
    "                              n_estimators=NUM_BASE_ESTIMATORS)\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_adaboost_classifier, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting ist ein weiterer Ensemble-Ansatz.\n",
    "Wie für AdaBoost wird eine Folge von schwachen Klassifizierern erstellt.\n",
    "Anders als bei AdaBoost wird jedoch nur die initiale Trainingsmenge zufällig erstellt und für sie ein Klassifizierer trainiert.\n",
    "Der Fehler dieses initialen Klassifizierers wird in den folgenden Schritten _numerisch_ ausgeglichen ohne dass weitere Klassifizierer trainiert werden.\n",
    "(Gradient Boosting ist keine Verallgemeinerung von AdaBoost.)\n",
    "\n",
    "Wie also trainiert Gradient Boosting den nächsten Stump S der Folge?\n",
    "Hehres Ziel ist eine Korrektur der Fehlklassifizierungen des bisher erstellten \"aktuellen\" Teil-Ensembles E zu korrigieren, d.h. der Abweichungen zwischen den aktuellen Vorhersagen von E und den tatsächlichen RednerInnen.\n",
    "Diese Abweichung wird mit einer sogenannten _Loss-Funktion_ L bewertet.\n",
    "Der Loss ist Null, wenn alle Reden korrekt klassifiziert werden, und größer als Null sonst.\n",
    "Außerdem wird L differenzierbar gewählt (z.B. mittlerer quadratischer Fehler über alle Reden), daher können wir die Steigung von L im Punkt der Klassifizierung des aktuellen Ensembles E berechnen - inklusive der Richtung des steilsten Abstiegs!\n",
    "Verändern wir die Vorhersage von E in diese Richtung, so können wir den Loss verringern und folglich eine verlässlichere Vorhersage erhalten.\n",
    "\n",
    "Wie sehr sollen wir in diese Richtung \"gehen\"?\n",
    "Die Schrittweite s sollten wir so optimieren, dass wir am niedrigsten Punkt landen.\n",
    "Aber Achtung:\n",
    "In der Richtung des steilsten Abstiegs liegt im Allgemeinen _nicht_ der Nullpunkt von L und es geht auch nicht ausschließlich \"bergab\".\n",
    "Daher ist die Schrittweite nicht beliebig.\n",
    "Dieser Abstieg gemäß Gradienten ist namensgebend für das Verfahren.\n",
    "\n",
    "<img src=\"images/20190103_141431.jpg\" width=\"50%\">\n",
    "\n",
    "Soweit die Idee.\n",
    "In der tatsächlichen Implementierung können wir aus dem gemachten Schritt \"bergab\" nicht einfach einen Decision Stump erzeugen.\n",
    "Daher wird in der Tat gar kein solcher Schritt vollzogen, sondern der nächste Stump S auf dem Gradienten von L im Punkt E trainiert (dieser Gradient wir auch Pseudo-Residual genannt).\n",
    "Anschließend können wir S zum bisherigen Ensemble \"einfach addieren\".\n",
    "\n",
    "Wichtig zu bemerken ist, dass die Komponenten des Gradientenvektors numerische Werte sind und keine Klassifizierung darstellen;\n",
    "Daher wird hier kein Klassifizierer, sondern ein Regressionsbaum trainiert.\n",
    "Weil der Stump S den Gradienten natürlich nicht perfekt abbilden kann, bedingt auch die Optimierung der Schrittweite s nicht unbedingt eine Verbesserung.\n",
    "Daher wird s nicht auf den Gradienten abgestimmt, sondern s verringert sich für jeden neuen Stump nach einem fest gegebenen Schema (im folgenden Beispiel haben wir die sogenannte _Learning Rate_ auf 0.6 gesetzt).\n",
    "Schließlich erhalten wir das nächste Teil-Ensemble \" E' = E + s\\*S \" und wiederholen mit E'.\n",
    "\n",
    "Alternativ zum gewohnten Training des ersten (Klassifikations-)Stumps der Folge kann dieser auch zufällig oder gar beliebig gewählt werden - der Ansatz der schrittweisen Fehlerkorrektur funktioniert auch dann.\n",
    "Die Vorhersage des finalen Ensembles besteht dann im Wesentlichen aus einer gewichteten Summe über alle Stumps.\n",
    "\n",
    "<img src=\"images/20190103_141437.jpg\" width=\"50%\">\n",
    "\n",
    "Mit Gradient Boosting können wir die Genauigkeit weiter auf ca. 94% heben, wofür wir allerdings ein größeres Ensemble als bisher benötigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "STEP_SIZE=0.6\n",
    "\n",
    "# Ein GradientBoostingClassifier verwendet standardmäßig Entscheidungsbäume.\n",
    "# Wir müssen noch die Höhe\n",
    "def generate_gradient_boosting_classifier():\n",
    "    return GradientBoostingClassifier(max_depth=DECISION_TREE_DEPTH,\n",
    "                                      n_estimators=NUM_BASE_ESTIMATORS*2,\n",
    "                                      learning_rate=STEP_SIZE)\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_gradient_boosting_classifier, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost stellt im Wesentlichen eine Verbesserung von herkömmlichem Gradient Boosting dar.\n",
    "Beispielsweise erlaubt die Verwendung von Gradienten zweiter Ordnung eine bessere Bestimmung der Abstiegsrichtung und -weite.\n",
    "Weiterhin werden Methoden zur besseren Vermeidung von Overfitting implementiert.\n",
    "Zudem arbeitet XGBoost deutlich schneller und parallelisiert die Berechnungen von sehr großen Entscheidungsbäumen.\n",
    "Für weiter Informationen und Hinweise, siehe das [codecentric.AI-Video](https://www.youtube.com/watch?v=xXZeVKP74ao) zum Thema XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "- [An intuitive explanation of gradient boosting](http://www.cse.chalmers.se/~richajo/dit865/files/gb_explainer.pdf)\n",
    "- [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "- [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "- [sklearn.ensemble.GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)\n",
    "- [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n",
    "- [Boosting Algorithms: AdaBoost, Gradient Boosting and XGBoost](https://hackernoon.com/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "nlp_basics.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "399.533px",
    "width": "431px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
