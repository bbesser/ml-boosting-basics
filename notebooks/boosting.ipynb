{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting für Machine Learning\n",
    "\n",
    "Wir beschäftigen uns mit den grundlegenden Varianten des sogenannten _Boostings_.\n",
    "Dabei legen wir unser Augenmerk auf Boosting für die Klassifikation.\n",
    "Insbesondere klassifizieren wir Reden von deutschen Politikern, d.h. aus einem gegebenen Redetext ermitteln wir wer diese Rede hielt.\n",
    "(Diese Problemstellung war bereits Thema im den Übungen des [codecentric.AI](https://www.codecentric.de/kuenstliche-intelligenz/) Bootcamp zum Thema [Natural Language Processing](https://www.youtube.com/watch?v=GmLsb-o7hvM).)\n",
    "\n",
    "_Angenommen für unser Problem steht ein schlechter Klassifikator K zur Verfügung._\n",
    "_Wie können wir aus K einen guten Klassifikator K' erzeugen?_\n",
    "\n",
    "Unter einem schlechten Klassifikator verstehen wir einen Klassifikator mit schlechter Vorhersagekraft, d.h. er macht viele Fehler.\n",
    "(Die Theorie erlaubt sogar so viele Fehler, dass die Kraft nur wenig besser als zufälliges Raten ist!)\n",
    "Der neue Klassifikator K' wird keine abgewandelte Form von K sein, sondern K' besteht aus mehreren geschickt kombinierten Instanzen von K.\n",
    "Daher bezeichnet man K' auch als _Ensemble_.\n",
    "\n",
    "Ensembles lassen sich für unterschiedliche zugrunde liegende Typen von Klassifikatoren erstellen.\n",
    "Hier beschränken wir uns auf den Fall, dass K ein einfacher [Entscheidungsbaum-Klassifikator](https://en.wikipedia.org/wiki/Decision_tree_learning) ist.\n",
    "Die Klassifikation des Ensembles K' ergibt sich aus den individuellen Klassifikationen der beteiligten Entscheidungsbäume.\n",
    "(Im Allgemeinen kann auch eine andere Sorte Klassifizierer zugrunde gelegt werden.)\n",
    "Im Rest des Artikels stellen wir verschiedene Verfahren je in einer kurzen allgemeinen Form vor, orientieren aber nähere Erläuterungen an der Implementierung mit Entscheidungsbäumen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten laden\n",
    "\n",
    "Werfen wir zuerst einen Blick auf die Daten.\n",
    "Wir verwenden den von Barbaresi und Adrien bereitgestellten Datensatz [1], der unter der URL [2] verfügbar ist.\n",
    "\n",
    "[1] Barbaresi, Adrien (2018). \"A corpus of German political speeches from the 21st century\", Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), European Language Resources Association (ELRA), pp. 792–797.\n",
    "\n",
    "[2] http://purl.org/corpus/german-speeches\n",
    "\n",
    "Hier ein Einblick in wenige zufällig ausgewählte Reden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run define_load_data_functions.ipynb\n",
    "\n",
    "df = load_data()\n",
    "\n",
    "display(df.sample(n=len(df)).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Datensatz ist Angela Merkel mit der weitaus stärksten Anzahl von Reden vertreten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "speech_counts = df.person.value_counts()\n",
    "\n",
    "display(speech_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beschränken uns auf die binäre Klassifikation, d.h. wir unterscheiden nur zwischen zwei Klassen von Personen.\n",
    "Um auf ausgewogenen Daten zu arbeiten wählen wir die Klassen _Angela Merkel_ und _Nicht Angela Merkel (Andere)_.\n",
    "Dazu vernachlässigen wir zuerst alle Personen, die mit nur wenigen Reden vertreten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NUM_SPEECHES = 100\n",
    "belowThreshold = lambda name: speech_counts[name] < MIN_NUM_SPEECHES\n",
    "\n",
    "dropped = filter(belowThreshold, speech_counts.index.tolist())\n",
    "df.drop(df[df.person.isin(dropped)].index, inplace=True)\n",
    "\n",
    "display(df.person.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann fassen wir alle verbleibenden Personen zur Klasse _Andere_ zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['person'] != \"Angela Merkel\", ['person']] = 'Andere'\n",
    "\n",
    "display(df.person.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vorverarbeiten\n",
    "\n",
    "Es folgt eine Vorverarbeitung der Reden mit Hilfe der NLP-Bibliothek [spaCy](https://spacy.io/).\n",
    "In diesem Schritt zerlegen wir jede Rede in ihre (durch Whitespace getrennten) Bestandteile, die sogenannten _Tokens_.\n",
    "In dem Zuge entfernen wir Tokens mit geringer Information, wie z.B. Interpunktion und Stoppwörter (\"und\", \"der\", \"die\", \"das\", ...).\n",
    "Außerdem überführen wir jedes Token in seine Grundform, das sogenannte _Lemma_ (z.B. \"angekündigt\" -> \"ankündigen\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%run init_spacy.ipynb\n",
    "%run define_preprocessing.ipynb\n",
    "\n",
    "df = load_cached_or_preprocess(df.speech)\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jede Rede R überführen wir nun die Folge von Lemmata von R in das \"Histogramm\" der Lemmahäufigkeiten von R.\n",
    "Bemerke, dass wir in diesem Schritt die der Reihenfolge Worte vergessen.\n",
    "(Man nennt solch eine vereinfachte Darstellung einer Wortfolge auch _bag of words_, um das Abhandensein von zeitlicher Information zu betonen.)\n",
    "\n",
    "Schauen wir uns ein paar Lemmata und die gewonnene Darstellung einer Rede im Detail an.\n",
    "(Einige Lemmata sind nicht \"perfekt\".\n",
    "Sie enthalten etwa Bindestriche oder liegen in gebeugter Form vor.\n",
    "Hier wird die Unschäfe der Sprache und ihrer Verarbeitung deutlich.\n",
    "Beispielsweise geschieht Lemmatisierung in spaCy nicht durch Anwendung eines Regelwerks, sondern durch neuronale Netze, die natürlich nicht in allen Fällen korrekte Ergebnisse erzielen.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%run define_feature_computation.ipynb\n",
    "\n",
    "(lemmata, lemma_index) = compute_index(df[\"lemmata\"])\n",
    "df[\"lemma_counts\"] = count(df[\"lemmata\"], lemma_index)\n",
    "\n",
    "display(lemmata[:10])\n",
    "display(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Bis hier haben wir aus den Reden die für das Modell-Training benötigten statistischen Informationen extrahiert.\n",
    "Zum Start des Trainings fehlt nur noch die Konvertierung diese Informationen in das benötigte Eingabedatenformat.\n",
    "\n",
    "Zum Training nutzen wir die ML-Bibliothek [scikit-learn](https://scikit-learn.org/stable/).\n",
    "Die eingegebenen Trainingdaten bestehen aus einer Sammlung von sogenannten _Labels_ (eins pro Rede) und sogenannten _Featurevektoren_ (ebenfalls einer pro Rede).\n",
    "In unserem Fall gibt das Label einer Rede R an, welche Person die Rede R hielt (\"Angela Merkel\" oder \"Andere\").\n",
    "Der Feature-Vektor von R kodiert das Histogramm der Lemmahäufigkeiten von R (haben wir oben bereits berechnet).\n",
    "\n",
    "Für die Eingabe in die Algorithmen von scikit-learn werden alle Labels in einen Vektor zusammengefasst.\n",
    "Analog werden alle Featurevektoren zu einer Matrix zusammengefasst.\n",
    "Dafür müssen natürlich alle Featurevektoren die gleiche Länge haben, was wir mit Hilfe der Funktion `dict_to_sparse` erledigen.\n",
    "(Insbesondere gibt diese Funktion eine sogenannte _dünn besetzte Matrix_ aus, in der Einträge mit Wert Null nicht explizit gespeichert werden.\n",
    "Warum?\n",
    "In einer Rede erscheinen bei Weitem nicht alle möglichen Lemmata, folglich enthält jeder Featurevektor viele Nullen.\n",
    "Wir sparen also viel Speicherplatz.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%run define_conversion_functions.ipynb\n",
    "\n",
    "feature_vectors = dict_to_sparse(df[\"lemma_counts\"], len(lemma_index))\n",
    "categories = df[\"person\"].astype(\"category\")\n",
    "labels = categories.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möge das Training beginnen.\n",
    "Erinnere, dass wir Ensembles von schwachen Klassifizierern erstellen möchten.\n",
    "Als schwachen Klassifizierer wählen wir einen sogenannten _Decision Stump_, d.h. einen Entscheidungsbaum der Tiefe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DECISION_TREE_DEPTH=1\n",
    "\n",
    "def generate_decision_stump():\n",
    "    return DecisionTreeClassifier(max_depth=DECISION_TREE_DEPTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Warmwerden wenden wir zunächst den schwachen Klassifizierer an.\n",
    "\n",
    "Dabei zerlegen wir die Eingabedaten in Trainingdaten und Testdaten, und zwar im üblichen Verhältnis 70/30.\n",
    "Nur die Trainingsdaten werden für das tatsächliche Training verwendet.\n",
    "Mit Hilfe der Testdaten wird die Performance des trainierten Modells evaluiert.\n",
    "\n",
    "Damit wir die Performance verlässich evaluieren können, führen wir das Experiment nicht nur einmal durch, sondern wiederholen es und mitteln die Ergebnisse.\n",
    "Die _Accuracy_ zeigt uns wie viele Testdatensätze korrekt klassifiziert wurden.\n",
    "Die sogenannte _Confusion Matrix_ schlüsselt die korrekten und falschen Klassifizierungen auf (in der i-ten Zeile und j-ten Spalte steht wie viele Reden von \"i\" als Reden von \"j\" klassifiziert wurden).\n",
    "Wir sehen, dass nicht einmal 80% der Testdaten korrekt klassifiziert werden, wobei mehr Reden von Angela Merkel falsch klassifiziert wurden (über 30%) als Reden von anderen (ca. 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run define_plot_functions.ipynb\n",
    "%run define_train_functions.ipynb\n",
    "\n",
    "TRAIN_TEST_RATIO = 0.3 # proportion of test data\n",
    "REPETITIONS = 20 # number of repetitions to average from\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_decision_stump, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein besseres Gefühl zu bekommen, schauen wir uns einen der trainierten Decision Stumps etwas genauer an.\n",
    "Interessant ist insbesondere die erste Zeile des Wurzelknotens.\n",
    "Die Häufigkeit eines Lemmas bestimmt welchem der beiden Blätter eine Rede zugeordnet wird.\n",
    "Jedes Blatt klassifiziert alle ihm zugeordneten Reden mit der gleichen Person, wie die jeweils letzte Zeile zeigt.\n",
    "(Die Werte _gini_, _samples_ und _value_ zeigen die [Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity), wie viele Reden dem jeweiligen Knoten zugeordnet werden bzw. die Redner-Verteilung aller dem Knoten zugeordneten Reden.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_decision_tree(classifiers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Wir haben Stumps unabhängig voneinander trainiert und ihre mittlere Performance betrachtet.\n",
    "Diese Idee wird beim sogenannten _Bagging_ systematisch angewendet, um einen Meta-Klassifizierer zu erzeugen.\n",
    "\n",
    "Hier trainieren wir 20 Decision Stumps unabhängig voneinander, wobei für jeden eine zufällige Teilmenge der Trainingsdaten verwendet wird.\n",
    "Es entsteht ein sogenanntes _Ensemble_ von Klassifizierern.\n",
    "(Da die Berechnungen der individuellen Stumps nicht voneinander abhängen, können wir mit Hilfe von allen zur Verfügung stehenden CPUs parallelisieren.)\n",
    "Das Bagging-Ensemble kombiniert die Vorhersagen der individuellen Stumps zu einer einzigen Vorhersage.\n",
    "Wenn die Person für Rede R vorhergesagt werden soll, wird nicht einfach nur unter allen individuellen Vorhersagen abgestimmt, sondern die individuellen Vorhersagewahrscheinlichkeiten beider Personen für R gemittelt und als Grundlage der finalen Vorhersage verwendet.\n",
    "\n",
    "Bemerke, dass wir keine Verbesserung der Performance gegenüber unserem ersten Experiment erzielen.\n",
    "Grund ist, dass jeder Bagging-Klassifizierer im Wesentlichen unsere Mittelung nachstellt.\n",
    "Die Mittelung über die Performances der Bagging-Klassifizierer verändert die einzelnen Ergebnisse also nicht entscheidend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "NUM_BASE_ESTIMATORS = 20\n",
    "ALL_CORES = -1\n",
    "\n",
    "def generate_bagging_classifier():\n",
    "    return BaggingClassifier(base_estimator=generate_decision_stump(),\n",
    "                             n_estimators=NUM_BASE_ESTIMATORS,\n",
    "                             n_jobs=ALL_CORES)\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_bagging_classifier, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Die Performance des schwachen Decision Stumps können wir sicherlich verbessern.\n",
    "Erfreulich wird sein, mit welch einfachen Mitteln das gelingt!\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) erstellt ebenfalls ein Ensemble von schwachen Klassifizierern.\n",
    "Diese sind im Gegensatz zum Bagging aber nicht unabhängig voneinander.\n",
    "Erstellt wird eine Sequenz von Klassifizierern, wobei sich der Fehler eines Klassifizierers auf das Training des nächsten auswirkt.\n",
    "Ziel ist es unter verschieden \"schwierigen\" Umständen zu trainieren und aus \"starken Sessions\" zu lernen.\n",
    "\n",
    "Wie trainiert AdaBoost in unserem Fall?\n",
    "Für jeden fertig trainierten Stump S werden falsch klassifizierte Reden identifiziert.\n",
    "Diese Reden stellen offenbar \"schwierige\" Eingaben für S dar - sonst würde sie ja von S korrekt erkannt.\n",
    "Solch schwierige Reden werden mit erhöhter Wahrscheinlichkeit ausgewählt, wenn eine zufällige Trainingsmenge für den nächsten Stump erzeugt wird.\n",
    "Das kommende Training ist also mit einer schwierigeren Eingabe konfrontiert, welche mehr Informationen über die Gesamtmenge aller Reden innehat.\n",
    "(Tatsächlich wird die Auswahlwahrscheinlichkeit _korrekt_ von S klassifizierter Reden _verringert_.\n",
    "Der Verringerungsfaktor ergibt sich aus dem _Fehler von S_, der Wahrscheinlichkeit eine zufällig gemäß aktueller Verteilung gezogene Rede falsch zu klassifizieren.)\n",
    "\n",
    "Wie klassifiziert AdaBoost?\n",
    "Jeder Stump des Ensembles geht mit unterschiedlichem Gewicht in die Gesamtvorhersage ein.\n",
    "Das Gewicht für Stump S ergibt sich aus dem Fehler von S:\n",
    "Je kleiner der Fehler, umso größeres Gewicht erhält S für die Gesamtvorhersage.\n",
    "\n",
    "Das Ergebnis unseres Experiments:\n",
    "Im Schnitt erreicht ein AdaBoost-Klassifier eine Genauigkeit von über 90% (mit Bagging erreichten wir nur unter 80%).\n",
    "Fehlklassifizierungen liegen nun im einstelligen Prozentbereich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# keine parallele Berechnung der Sequenz möglich\n",
    "def generate_adaboost_classifier():\n",
    "    return AdaBoostClassifier(base_estimator=generate_decision_stump(),\n",
    "                              n_estimators=NUM_BASE_ESTIMATORS)\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_adaboost_classifier, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting ist ein weiterer Ensemble-Ansatz.\n",
    "Wie für AdaBoost wird eine Folge von schwachen Klassifizierern erstellt.\n",
    "Anders als bei AdaBoost wird jedoch nur die initiale Trainingsmenge zufällig erstellt.\n",
    "(Gradient Boosting ist keine Verallgemeinerung von AdaBoost.)\n",
    "\n",
    "Wie also trainiert Gradient Boosting den nächsten Stump S der Folge?\n",
    "Hehres Ziel ist es die Fehlklassifizierungen des bisher erstellten \"aktuellen\" Teil-Ensembles E zu korrigieren.\n",
    "Die Fehlklassifizierungen von E, d.h. die Abweichungen zwischen den aktuellen Vorhersagen von E und den tatsächlichen RednerInnen, werden in einer sogenannten _Loss-Funktion_ L zusammengefasst.\n",
    "Der Loss ist Null, wenn alle Reden korrekt klassifiziert werden, und größer als Null sonst.\n",
    "Die Funktion L wird differenzierbar gewählt, daher können wir die Steigung von L für die aktuelle Klassifizierung berechnen - inklusive der Richtung des steilsten Abstiegs!\n",
    "In diese Richtung wollen wir \"gehen\" und dabei die Schrittweite s so optimieren, dass wir am niedrigsten Punkt landen.\n",
    "(Achtung: In der Richtung des steilsten Abstiegs liegt im Allgemeinen _nicht_ der Nullpunkt von L und es geht auch nicht ausschließlich \"bergab\".\n",
    "Daher ist die Schrittweite nicht beliebig.)\n",
    "Dieser Abstieg gemäß Gradienten ist namensgebend für das Verfahren.\n",
    "\n",
    "Soweit die Idee.\n",
    "In der tatsächlichen Implementierung können wir aus dem gemachten Schritt bergab nicht einfach einen Decision Stump erzeugen.\n",
    "Daher wird der nächste Stump S auf dem Gradienten von L im Punkt E trainiert (dieser Gradient wir auch Pseudo-Residual genannt).\n",
    "Schließlich erhalten wir das nächste Teil-Ensemble \" E' = E + S \" und wiederholen.\n",
    "Die Schrittweite wird nicht optimiert, sondern verringert sich für jeden neuen Stump nach einem fest gegebenen Schema (im Beispiel haben wir Faktor 0.6 gewählt).\n",
    "\n",
    "Der erste Stump der Folge kann beliebig gewählt werden oder er wird wie gewohnt trainiert.\n",
    "Die Vorhersage des finalen Ensembles besteht dann im Wesentlichen aus einer gewichteten Summe über alle Stumps.\n",
    "\n",
    "Mit Gradient Boosting können wir die Genauigkeit weiter auf fast 93% heben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "STEP_SIZE=0.6\n",
    "\n",
    "# Ein GradientBoostingClassifier verwendet standardmäßig Entscheidungsbäume.\n",
    "# Wir müssen noch die Höhe\n",
    "def generate_gradient_boosting_classifier():\n",
    "    return GradientBoostingClassifier(max_depth=DECISION_TREE_DEPTH,\n",
    "                                      n_estimators=NUM_BASE_ESTIMATORS,\n",
    "                                      learning_rate=STEP_SIZE)\n",
    "\n",
    "(classifiers, accuracies, confusion_matrices) = train_and_test_repeated(\n",
    "    classifier_generator=generate_gradient_boosting_classifier, \n",
    "    data=feature_vectors,\n",
    "    labels=labels,\n",
    "    test_size=TRAIN_TEST_RATIO,\n",
    "    repetitions=REPETITIONS)\n",
    "\n",
    "display_mean_accuracy(accuracies)\n",
    "display_mean_confusion_matrix(confusion_matrices, classes=categories.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "\n",
    "base = make_decision_stump()\n",
    "classifier = XGBClassifier(max_depth=1,\n",
    "                           n_estimators=BOOST_FACTOR)\n",
    "\n",
    "(accuracy, confusion) = train_and_test_with(classifier,\n",
    "                                            feature_vectors,\n",
    "                                            labels,\n",
    "                                            test_size=TRAIN_TEST_RATIO)\n",
    "\n",
    "display(accuracy)\n",
    "plot_confusion_matrix(confusion, classes=categories.unique())\n",
    "display(graphviz.Source(xgboost.to_graphviz(classifier)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "- [An intuitive explanation of gradient boosting](http://www.cse.chalmers.se/~richajo/dit865/files/gb_explainer.pdf)\n",
    "- [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "- [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)\n",
    "- [sklearn.ensemble.GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)\n",
    "- [A Gentle Introduction to Gradient Boosting](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)\n",
    "- [Boosting Algorithms: AdaBoost, Gradient Boosting and XGBoost](https://hackernoon.com/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "nlp_basics.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "399.533px",
    "width": "431px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
